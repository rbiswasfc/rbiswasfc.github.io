<!DOCTYPE html>
<html lang="en">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.55.6" />

    
    
    

<title>Neural Machine Translation using Encoder-Decoder Networks • Raja Biswas</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Neural Machine Translation using Encoder-Decoder Networks"/>
<meta name="twitter:description" content="&ndash; Project Status: Active In this project, I aim at building different sequence to sequence machine learning models for Spanish to English translation task using PyTorch. The Sequence to sequence or &ldquo;Seq2Seq&rdquo; is an end-to-end model comprising of two recurrent neural networks:
 An Encoder: takes sentences in the source language as input and encodes them into finite dimensional context vectors;
 A Decoder: uses the context vector as a seed from which the translated sentences are generated."/>

<meta property="og:title" content="Neural Machine Translation using Encoder-Decoder Networks" />
<meta property="og:description" content="&ndash; Project Status: Active In this project, I aim at building different sequence to sequence machine learning models for Spanish to English translation task using PyTorch. The Sequence to sequence or &ldquo;Seq2Seq&rdquo; is an end-to-end model comprising of two recurrent neural networks:
 An Encoder: takes sentences in the source language as input and encodes them into finite dimensional context vectors;
 A Decoder: uses the context vector as a seed from which the translated sentences are generated." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://rbiswasfc.github.io/posts/projects/machine_translation/" />
<meta property="article:published_time" content="2019-07-12T21:09:48&#43;08:00"/>
<meta property="article:modified_time" content="2019-07-12T21:09:48&#43;08:00"/><meta property="og:site_name" content="IMbbb" />


    


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">








<link rel="stylesheet" href="/scss/hyde-hyde.05f0e7f260ab4d576b9e72741355db3eecf1f8aafd65567cb02efb01c44d96f0.css" integrity="sha256-BfDn8mCrTVdrnnJ0E1XbPuzx&#43;Kr9ZVZ8sC77AcRNlvA=">


<link rel="stylesheet" href="/scss/print.2744dcbf8a0b2e74f8a50e4b34e5f441be7cf93cc7de27029121c6a09f9e77bc.css" integrity="sha256-J0Tcv4oLLnT4pQ5LNOX0Qb58&#43;TzH3icCkSHGoJ&#43;ed7w=" media="print">



    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png">
    
</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://rbiswasfc.github.io/">Raja Biswas</a>
      </span>
      
        
        
        
        <div class="author-image">
          <img src="https://rbiswasfc.github.io/img/raja_profile.JPG" alt="Author Image" class="img--circle img--headshot element--center">
        </div>
        
      
      
      <p class="site__description">
        
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Raja Biswas</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/about/">
						<span>About</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/posts/">
						<span>Posts</span>
					</a>
				</li>
			 
		
	</ul>
</div>

        <section class="social">
	
	<a href="https://github.com/rbiswasfc" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	<a href="https://linkedin.com/in/raja-biswas" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	<a href="https://instagram.com/_rbiswas" rel="me"><i class="fab fa-instagram fa-lg" aria-hidden="true"></i></a>
	
	
	<a href="https://twitter.com/raja_biswas" rel="me"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	


	
	
	
	
	
	
	
</section>

      </div>
    </div>
    

<div class="builtwith">
Built with <a href="https://gohugo.io">Hugo</a> ❤️ <a href="https://github.com/htr3n/hyde-hyde">hyde-hyde</a>.
</div>


  </div>
</div>

        <div class="content container">
            
    


<article>
  <header>
    <h1>Neural Machine Translation using Encoder-Decoder Networks</h1>
    
    
<div class="post__meta">
    
    
      <i class="fas fa-calendar-alt"></i> Jul 12, 2019
    
    
    
    
    
      
      
          <br/>
           <i class="fas fa-tags"></i>
          
          <a class="badge badge-tag" href="/tags/nlp">nlp</a>
           
      
          <a class="badge badge-tag" href="/tags/pytorch">pytorch</a>
          
      
    
    
    <br/>
    <i class="fas fa-clock"></i> 3 min read
</div>


  </header>
  
  
  <div class="post">
    

<h4 id="project-status-active">&ndash; Project Status: Active</h4>

<p>In this project, I aim at building different sequence to sequence machine learning models for Spanish to English translation task using PyTorch. The Sequence to sequence or &ldquo;Seq2Seq&rdquo;  is an end-to-end model comprising of two recurrent neural networks:</p>

<ul>
<li><p>An Encoder: takes sentences in the source language as input and encodes them into finite dimensional context vectors;</p></li>

<li><p>A Decoder: uses the context vector as a seed from which the translated sentences are generated.</p></li>
</ul>

<p>For the NMT task, I have adopted four different Seq2Seq architectures:</p>

<ul>
<li>LSTM Encoder-Decoder</li>
<li>LSTM Encoder-Decoder with attention</li>
<li>Sub-word modelling with character level CNN</li>
<li>The Transformer model</li>
</ul>

<p>Brief explanations of each approaches are provided in the following.</p>

<h4 id="lstm-encoder-decoder-network-https-github-com-rbiswasfc-machine-translation-blob-master-vanilla-lstm-lstm-encoder-decoder-vanilla-ipynb"><a href="https://github.com/rbiswasfc/Machine-Translation/blob/master/Vanilla-LSTM/LSTM-Encoder-Decoder-Vanilla.ipynb">LSTM Encoder-Decoder Network</a></h4>

<p>The architecture of LSTM Encoder-Decoder Network is illustrated below</p>

<p><img src="/img/NMT/Vanilla_LSTM.PNG" alt="NMT" /></p>

<p>A bi-directional LSTM is used as Encoder, since  a word in a sentence can have a dependency on another word before or after it.
The encoder encapsulates information in the source sentence into a context vector, which acts as initial hidden state for the Decoder network.
The implementation of the model is elaborated in this <a href="https://github.com/rbiswasfc/Machine-Translation/blob/master/Vanilla-LSTM/LSTM-Encoder-Decoder-Vanilla.ipynb">Jupyter Notebook</a>.</p>

<h5 id="remarks">Remarks</h5>

<ul>
<li><p>It is difficult to compress an arbitrary-length source sequence into
a single fixed-size context vector. The issue can be mitigated by building Encoder with stacked LSTM layers: each layer’s outputs are the input sequence to
the next layer.</p></li>

<li><p>Seq2Seq models are known to lose effectiveness on very long inputs, a consequence of the practical limits of
LSTMs. Encoder-Decoder network with attention mechanism can help in capturing long term dependency.</p></li>
</ul>

<h4 id="lstm-encoder-decoder-with-attention-https-github-com-rbiswasfc-machine-translation-blob-master-lstm-with-attention-encoder-decoder-w-attention-ipynb"><a href="https://github.com/rbiswasfc/Machine-Translation/blob/master/LSTM-with-attention/Encoder_Decoder_w_Attention.ipynb">LSTM Encoder-Decoder with Attention</a></h4>

<ul>
<li>Global Attention Model (Luong, et al. 2015)</li>
</ul>

<h5 id="remarks-1">Remarks</h5>

<ul>
<li>Needs better handling of <UNK> tokens generated during translation</li>
<li>Cannot use full GPU acceleration due to auto-regressive nature</li>
</ul>

<h4 id="sub-word-modelling-with-character-level-cnn-https-github-com-rbiswasfc-machine-translation-blob-master-sub-word-machine-translation-char-based-ipynb"><a href="https://github.com/rbiswasfc/Machine-Translation/blob/master/sub-word/Machine-Translation-Char-Based.ipynb">Sub-word modelling with character level CNN</a></h4>

<p>Hybrid model. Better UNK handling.</p>

<h5 id="remarks-2">Remarks</h5>

<ul>
<li>Computationally expensive</li>
</ul>

<h4 id="the-transformer-model-https-github-com-rbiswasfc-machine-translation-blob-master-the-transformer-thetransformer-ipynb"><a href="https://github.com/rbiswasfc/Machine-Translation/blob/master/the-transformer/TheTransformer.ipynb">The Transformer model</a></h4>

<p>tbc</p>

<h5 id="remarks-3">Remarks</h5>

<ul>
<li>It can utilize full GPU acceleration</li>
</ul>

<h3 id="training">Training</h3>

<h4 id="dataset">Dataset</h4>

<p>The tarin, dev and test dataset are located here. It contains the follwing files</p>

<h4 id="initialization-and-optimization">Initialization and Optimization</h4>

<p>Uniform initialization. Adam optimizer. The model is trained in GPU in google colab.</p>

<h3 id="evaluation">Evaluation</h3>

<p>BLEU is used to evaluate the model performance. The following scores are achieved</p>

<ul>
<li>LSTM</li>
<li>Attn</li>
<li>Sub-word</li>
<li>Transformer: Needs to train</li>
</ul>

<h3 id="to-do-list">To-do list</h3>

<ul>
<li>Train the transformer</li>
<li>Investigate different initialization</li>
<li>Build stacked LSTM encoder-decoder with attention</li>
<li>Plot attention and demonstrate alignment during translation</li>
<li>Learning rate scheduler</li>
<li>Use the model architecture for other open-source machine translation datasets</li>
</ul>

<h3 id="credits">Credits</h3>

<p>This project uses the public resources provided in <a href="http://web.stanford.edu/class/cs224n/"><em>CS224n: Natural Language Processing with Deep Learning</em></a> Stanford / Winter 2019 course. I am thankful to the lecturers and TAs for offering such amazing contents. I believe this course is one of the best places to learn NLP in 2019.</p>

<h3 id="contact">Contact</h3>

<ul>
<li>Raja Biswas: rajabiswas92@outlook.com</li>
</ul>

  </div>
  

<div class="navigation navigation-single">
    
    <a href="/posts/projects/ndsc/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">National Data Science Challenge 2019 - Shopee</span>
    </a>
    
    
    <a href="/posts/projects/deepfly/" class="navigation-next">
      <span class="navigation-tittle">A bottom-up recommender system for travel destinations</span>
      <i aria-hidden="true" class="fa fa-chevron-right"></i>
    </a>
    
</div>


  

  
    


</article>


        </div>
        
    

<script defer src="https://use.fontawesome.com/releases/v5.5.0/js/all.js" integrity="sha384-GqVMZRt5Gn7tB9D9q7ONtcp4gtHIUEW/yG7h98J7IpE3kpi+srfFyyB/04OV6pG0" crossorigin="anonymous"></script>


    
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.13.1/highlight.min.js"></script>
        
    
    <script type="text/javascript">
        
        hljs.initHighlightingOnLoad();
    </script>
    




    



    </body>
</html>
